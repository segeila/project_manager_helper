{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Manager Helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem\n",
    "\n",
    "Over the years there were numerous projects started inside Nokia. Information about these projects exists in unstructured text format (*.doc,* *.pdf,* *.ppt* and other formats). Additionally, companies that have been aquired by Nokia also have a similar project descriptions.\n",
    "\n",
    "When planning to start a new project, project manager would like to get an overview of existing projects within the company that are the most similar to his project idea. Useful information can include statistics on team size, budget, timeline. As well as a spectre of people, technologies, partner companies and development tools being presented in such projects.\n",
    "\n",
    "Proposed solution\n",
    "\n",
    "One way to solve this problem requitres splitting it into two smaller procedures.\n",
    "\n",
    "Step 1: Narrow down the whole set of documents to the relevant subset describing the same technology.\n",
    "        This can be solved by extracting keywords from an input document and comparing them to the keywords found\n",
    "        in each document in a database of previous projects.\n",
    "        IBM Watson Natural Language Understanding service can be used to extract keywords.\n",
    "\n",
    "        Input: brief description of a project given in a plain text\n",
    "        Output: list of the most similar project descriptions. Additionally a similarity score can be provided.\n",
    "        Search space: all documents in a database.\n",
    "  \n",
    "Step 2: Discover patterns, dependencies and relationships within narrowed subset of the documents.\n",
    "        IBM Watson Discovery service can be used to conduct such analysis.\n",
    "        Input: subset of documents\n",
    "        Output: statistics, relations, etc.\n",
    "        \n",
    "This notebook explains the first step of the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is no existing dataset that can be used, we will be working with a NIPS dataset. It consists of 403 research papers presented during the annual NIPS conference which covers topics of AI, machine and deep learning, data scienece, etc.\n",
    "Additionally, a .csv file is provided which includes title of each article and its full text.\n",
    "\n",
    "For the first step we will use .csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing required modules. Make sure to install them locally beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "from watson_developer_cloud import NaturalLanguageUnderstandingV1\n",
    "from watson_developer_cloud.natural_language_understanding_v1 import Features, KeywordsOptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an instance of a service. Version, username and password can be found in 'View Credentials' tab for this specific service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "    version='2017-02-27',\n",
    "    username='ff57ed6a-a245-40bf-b6a0-a709612b9a7e',\n",
    "    password='FOEh24Cuwubq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading csv file with article text and titles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Name of the file to read as a string\n",
    "csv_file = 'Papers.csv'\n",
    "\n",
    "#Use pandas module to read csv file\n",
    "articles = pd.read_csv(csv_file)\n",
    "\n",
    "#Choosing only Title and PaperTExt columns.\n",
    "articles = articles.loc[:,['Title', 'PaperText']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if out file was succsesfully read. Let's output first 5 titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Double or Nothing: Multiplicative Incentive Me...\n",
      "1    Learning with Symmetric Label Noise: The Impor...\n",
      "2     Algorithmic Stability and Uniform Generalization\n",
      "3    Adaptive Low-Complexity Sequential Inference f...\n",
      "4    Covariance-Controlled Adaptive Langevin Thermo...\n",
      "Name: Title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print articles.Title.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating variables to store keywords and titles for our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset includes 403 articles.\n"
     ]
    }
   ],
   "source": [
    "#Empty array to store a set of keywords for each article as a separate element\n",
    "doc_vectors = []\n",
    "\n",
    "#Empty array to store titles\n",
    "titles = []\n",
    "\n",
    "#Amount of articles in the dataset (= amount of rows)\n",
    "size = len(articles.index)\n",
    "print 'Dataset includes %d articles.' % size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will iterate over every article in a dataset and extract keywords for that article using IBM Watson Natural Language Understanding service an instance of which we have already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting keywords for document 0 out of 403\n",
      "Extracting keywords for document 10 out of 403\n",
      "Extracting keywords for document 20 out of 403\n",
      "Extracting keywords for document 30 out of 403\n",
      "Extracting keywords for document 40 out of 403\n",
      "Extracting keywords for document 50 out of 403\n",
      "Extracting keywords for document 60 out of 403\n",
      "Extracting keywords for document 70 out of 403\n",
      "Extracting keywords for document 80 out of 403\n",
      "Extracting keywords for document 90 out of 403\n",
      "Extracting keywords for document 100 out of 403\n",
      "Extracting keywords for document 110 out of 403\n",
      "Extracting keywords for document 120 out of 403\n",
      "unknown language detected\n",
      "Extracting keywords for document 130 out of 403\n",
      "Extracting keywords for document 140 out of 403\n",
      "Extracting keywords for document 150 out of 403\n",
      "Extracting keywords for document 160 out of 403\n",
      "Extracting keywords for document 170 out of 403\n",
      "Extracting keywords for document 180 out of 403\n",
      "Extracting keywords for document 190 out of 403\n",
      "Extracting keywords for document 200 out of 403\n",
      "Extracting keywords for document 210 out of 403\n",
      "Extracting keywords for document 220 out of 403\n",
      "Extracting keywords for document 230 out of 403\n",
      "Extracting keywords for document 240 out of 403\n",
      "Extracting keywords for document 250 out of 403\n",
      "Extracting keywords for document 260 out of 403\n",
      "Extracting keywords for document 270 out of 403\n",
      "Extracting keywords for document 280 out of 403\n",
      "Extracting keywords for document 290 out of 403\n",
      "Extracting keywords for document 300 out of 403\n",
      "Extracting keywords for document 310 out of 403\n",
      "Extracting keywords for document 320 out of 403\n",
      "Extracting keywords for document 330 out of 403\n",
      "Extracting keywords for document 340 out of 403\n",
      "Extracting keywords for document 350 out of 403\n",
      "Extracting keywords for document 360 out of 403\n",
      "Extracting keywords for document 370 out of 403\n",
      "Extracting keywords for document 380 out of 403\n",
      "Extracting keywords for document 390 out of 403\n",
      "Extracting keywords for document 400 out of 403\n"
     ]
    }
   ],
   "source": [
    "#Iterate over each article, keep track of its index\n",
    "for index, row in articles.iterrows():\n",
    "    #Create an empty array to populate it with \n",
    "    doc_vector = []\n",
    "    \n",
    "    #Extract title and append it to the list of titles\n",
    "    title = row['Title']\n",
    "    titles.append(title)\n",
    "    \n",
    "    #Extract text\n",
    "    text = row['PaperText']\n",
    "    \n",
    "    #To keep track of the process we will print out index of a document being analyzed at the moment\n",
    "    if index % 10 == 0:\n",
    "        print 'Extracting keywords for document %d out of %d' % (index, size)\n",
    "        \n",
    "    #Use try-except to catch possible errors \n",
    "    try:\n",
    "        #Feed text to an instance of IBM Watson NLU service. We ask to return only keywords.\n",
    "        keywords = natural_language_understanding.analyze(text=text, features=Features(keywords=KeywordsOptions()))['keywords']\n",
    "    except Exception as e:\n",
    "        print e.message\n",
    "        pass\n",
    "    \n",
    "    #Service returns a list of keywords and relevance scores but we only need keywords\n",
    "    for keyword in keywords:\n",
    "        doc_vector.append(keyword['text'])\n",
    "        \n",
    "    #Append keywords extracted from an article to the list of keywords\n",
    "    doc_vectors.append(doc_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add extracted keywords as a new column\n",
    "word_vector = pd.Series(doc_vectors)\n",
    "articles['word_vector'] = word_vector.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check results of keyword extraction. Change index to any number in range 0-402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Bidirectional Recurrent Convolutional Networks for Multi-Frame Super-Resolution\"\n",
      "\n",
      "Extracted keywords: \n",
      "\n",
      "temporal dependency\n",
      "hidden layer\n",
      "recurrent convolutional network\n",
      "conditional convolutions\n",
      "multi-frame sr\n",
      "feedforward convolution\n",
      "SR methods\n",
      "multi-frame SR methods\n",
      "single-image sr\n",
      "PSNR Time\n",
      "current hidden layer\n",
      "BRCN\n",
      "recurrent neural networks\n",
      "input layer\n",
      "recurrent convolutional sub-network\n",
      "Recurrent Convolutional Networks\n",
      "conditional convolution\n",
      "optical flow\n",
      "video SR\n",
      "temporal dependency modelling\n",
      "bidirectional scheme\n",
      "complex motions\n",
      "video sequences\n",
      "Feedforward convolution models\n",
      "motion estimation\n",
      "backward recurrent network\n",
      "convolutional neural network\n",
      "video frames\n",
      "multi-frame SR method\n",
      "forward recurrent network\n",
      "bidirectional recurrent scheme\n",
      "conditional convolutional connections\n",
      "feedforward convolution focus\n",
      "visual spatial dependency\n",
      "single-image SR methods\n",
      "video SR methods\n",
      "recurrent convolutions\n",
      "powerful temporal dependency\n",
      "Information Processing Systems\n",
      "efficient multi-frame SR\n",
      "IEEE International Conference\n",
      "output layer\n",
      "single-image SR method\n",
      "multiframe SR methods\n",
      "convolution connects input\n",
      "computational cost\n",
      "cheap convolution operations\n",
      "Neural Information Processing\n",
      "high computational cost\n"
     ]
    }
   ],
   "source": [
    "print '\"%s\"' % articles.loc[26, 'Title']\n",
    "print ''\n",
    "print 'Extracted keywords: '\n",
    "print ''\n",
    "for keyword in articles.loc[26, 'word_vector']:\n",
    "    print keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will read an imaginary description of a new project. You can write your own and put it in the root folder inside new_project.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('new_project.txt', 'r') as f:\n",
    "    text_to_compare = f.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espoo Karage is a new space for all Nokia employees. They can learn and practice new skills and technologies such as artificial intelligence, machine learning, deep learning and data analysis. Working stations, various sensors and cameras are provided for a free and unlimited use. The use of Karage will be analyzed using low resolution thermal and digital cameras, feedback system based on IBM Watson sentimental analysis module. Convolutional neural networks will be applied to images from cameras for object detection, image segmentation and image processing. Data extracted from video stream is used to make prediction about utilization of the space in upcoming days. Time series are analied using RNN and linear regression models. Accuracy of these models is compared based on ROC AUC and cross enthropy. Results of this analysis will be presented to the manager of the Karage so he can access how successful this project is and what improvements can be made.\n"
     ]
    }
   ],
   "source": [
    "print text_to_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply the same procedure to extract keywords from that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords found in a document:\n",
      "\n",
      "Convolutional neural networks\n",
      "Watson sentimental analysis\n",
      "linear regression models\n",
      "Espoo Karage\n",
      "ROC AUC\n",
      "Nokia employees\n",
      "digital cameras\n",
      "new space\n",
      "artificial intelligence\n",
      "image segmentation\n",
      "various sensors\n",
      "Working stations\n",
      "new skills\n",
      "machine learning\n",
      "deep learning\n",
      "low resolution\n",
      "data analysis\n",
      "object detection\n",
      "video stream\n",
      "image processing\n",
      "Time series\n",
      "utilization\n",
      "improvements\n",
      "Accuracy\n",
      "prediction\n",
      "module\n",
      "technologies\n",
      "IBM\n",
      "images\n",
      "Results\n",
      "manager\n"
     ]
    }
   ],
   "source": [
    "keywords_to_compare = [i['text'] for i in natural_language_understanding.analyze(text=text_to_compare, features=Features(keywords=KeywordsOptions()))['keywords']]\n",
    "\n",
    "print 'Keywords found in a document:' + '\\n'\n",
    "\n",
    "for keyword in keywords_to_compare:\n",
    "    print keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare similarity of two given sets of keywords we will use a fairly naive approach which calculates an intersection of two sets of keywords. The bigger is that intesection the higher is the similarity between two documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naive_similarity(word_vector01, word_vector02):\n",
    "    n = len(word_vector01)\n",
    "    counter = 0\n",
    "    matches = []\n",
    "    for word in word_vector01:\n",
    "        if word in word_vector02:\n",
    "            counter += 1\n",
    "            matches.append(word)\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will loop over all sets of keywords and find similarity with a given document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = []\n",
    "\n",
    "for index, row in articles.iterrows():\n",
    "    temp_similarity = naive_similarity(keywords_to_compare, row['word_vector'])\n",
    "    similarities.append(temp_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article with the highest similarity is \"Variational Dropout and the Local Reparameterization Trick\"\n",
      "\n",
      "Number of matching keywords are:\n",
      "\n",
      "deep learning\n",
      "low resolution\n",
      "image processing\n"
     ]
    }
   ],
   "source": [
    "max_similarity = similarities.index(max(similarities))\n",
    "\n",
    "print'Article with the highest similarity is \"%s\"' % articles.ix[index, 'Title'] + '\\n'\n",
    "print'Number of matching keywords are:' + '\\n'\n",
    "\n",
    "matches = set(keywords_to_compare).intersection(articles.ix[max_similarity, 'word_vector'])\n",
    "\n",
    "for word in matches:\n",
    "    print word"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
